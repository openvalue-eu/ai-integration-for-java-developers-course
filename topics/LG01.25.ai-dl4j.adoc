== Deep Learning with Deeplearning4J
:imagesdir: ../images
:topic: 5

[.notes]
--
_topicStart:Module-{module}-Topic-{topic}-Day-{day}_
--

=== DL4J Overview and Architecture

[.notes]
--
**Instructor Notes:** Position DL4J for teams wanting pure Java deep learning solution.
--

**DL4J Ecosystem:**
[%step]
--
- **ND4J:** N-dimensional arrays (like NumPy for Java)
- **DataVec:** ETL and data preprocessing
- **DL4J:** Neural network training and inference
- **RL4J:** Reinforcement learning
- **Arbiter:** Hyperparameter optimization
--

**Best Use Cases:**
[%step]
--
- Custom neural network architectures
- Computer vision applications
- NLP with custom models
- Distributed training with Spark
- Pure Java stack requirement
--

=== Setting Up Deeplearning4j

[.notes]
--
**Instructor Notes:** Walk through Maven setup. Explain CPU vs GPU backend choice.
--

**Core Dependencies:**
```xml
<!-- DL4J Core -->
<dependency>
    <groupId>org.deeplearning4j</groupId>
    <artifactId>deeplearning4j-core</artifactId>
    <version>1.0.0-M2.1</version>
</dependency>

<!-- ND4J Backend - Choose ONE -->
<!-- For CPU -->
<dependency>
    <groupId>org.nd4j</groupId>
    <artifactId>nd4j-native-platform</artifactId>
    <version>1.0.0-M2.1</version>
</dependency>

<!-- OR for GPU (requires CUDA) -->
<dependency>
    <groupId>org.nd4j</groupId>
    <artifactId>nd4j-cuda-11.6-platform</artifactId>
    <version>1.0.0-M2.1</version>
</dependency>

<!-- DataVec for data processing -->
<dependency>
    <groupId>org.datavec</groupId>
    <artifactId>datavec-api</artifactId>
    <version>1.0.0-M2.1</version>
</dependency>
```

**Backend Selection:**
[%step]
--
- **CPU (nd4j-native):** Easier setup, works everywhere
- **GPU (nd4j-cuda):** Much faster training, requires NVIDIA GPU
- Platform suffix includes all OS-specific binaries
--

=== Understanding ND4J Arrays

**Instructor Notes:** Explain ND4J as foundation. Show similarity to NumPy for Python developers.

**Key Concepts:**
[%step]
--
- `INDArray`: N-dimensional array (tensors)
- Operations: element-wise, matrix multiplication, broadcasting
- Memory management: off-heap storage for large arrays
- GPU acceleration support
--

**Common Operations:**
```java
// Create array
INDArray array = Nd4j.create(new float[]{1, 2, 3, 4});

// Reshape
INDArray matrix = array.reshape(2, 2);

// Math operations
INDArray result = matrix.mul(2); // Multiply by 2
```

=== Building Neural Networks with DL4J

**Instructor Notes:** Show basic network configuration. Explain layer types and activation functions.

**Network Configuration Pattern:**
```java
MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
    .seed(123)
    .updater(new Adam(0.001))
    .list()
    .layer(new DenseLayer.Builder()
        .nIn(784)
        .nOut(256)
        .activation(Activation.RELU)
        .build())
    .layer(new OutputLayer.Builder()
        .nIn(256)
        .nOut(10)
        .activation(Activation.SOFTMAX)
        .lossFunction(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
        .build())
    .build();
```

**Key Configuration Elements:**
[%step]
--
- **Seed:** Reproducibility
- **Updater:** Optimization algorithm (Adam, SGD, RMSProp)
- **Layers:** Input → Hidden → Output
- **Activation:** Non-linearity (ReLU, Sigmoid, Tanh)
- **Loss Function:** Training objective
--

=== Common Layer Types

**Instructor Notes:** Explain when to use each layer type.

==== Layer Types:

**Dense (Fully Connected):**
[%step]
--
- Every neuron connects to all neurons in next layer
- Use for: general purpose, tabular data
- Configuration: nIn, nOut, activation
--

**Convolutional (Conv2D):**
[%step]
--
- For spatial data (images)
- Learns local patterns
- Configuration: kernel size, stride, padding
--

**LSTM (Long Short-Term Memory):**
[%step]
--
- For sequential data (time series, text)
- Remembers long-term dependencies
- Configuration: nIn, nOut, forget gate bias
--

**Dropout:**
[%step]
--
- Regularization technique
- Randomly drops neurons during training
- Configuration: dropout rate (0.2-0.5)
--

**Output Layer:**
[%step]
--
- Final layer producing predictions
- Activation depends on task (Softmax for classification, Identity for regression)
--